import settings

import os
import json
import re
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
from harvesttext import HarvestText
import zhconv
from itertools import cycle


class Config:
    # !!!íŒŒì¼ ê²½ë¡œ ìˆ˜ì •í•´ì•¼í•¨!!!!!
    TRAIN_FILE = str(settings.DATA_HOME / "train/usual_train.txt")
    EVAL_FILE = str(settings.DATA_HOME / "eval/usual_eval_labeled.txt")

    # ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
    MODEL_NAME = "bert-base-chinese"
    NUM_LABELS = 6  # (ë°ì´í„° ë¡œë”© ì‹œ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨)
    MAX_LEN = 128  # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
    BATCH_SIZE = 32  # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì—ëŸ¬ë‚˜ë©´ 16ìœ¼ë¡œ ì¤„ì´ê¸°)
    EPOCHS = 4  # í•™ìŠµ ì—í­ ìˆ˜
    LEARNING_RATE = 2e-5  # í•™ìŠµë¥  (BERT ë¯¸ì„¸ì¡°ì • í‘œì¤€ê°’)

    # ì¥ì¹˜ ì„¤ì • (GPU ìš°ì„  ì‚¬ìš©)
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    SAVE_PREFIX = settings.MODEL_HOME / "bert"  # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ íŒŒì¼ëª…


config = Config()


def remove_url(src):
    """URL ë§í¬ ì œê±°"""
    vTEXT = re.sub(
        r"(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b",
        "",
        src,
        flags=re.MULTILINE,
    )
    return vTEXT


def preprocess_data(file_path):
    """íŒŒì¼ì„ ì½ì–´ì„œ ì „ì²˜ë¦¬(ë²ˆì²´->ê°„ì²´, ë…¸ì´ì¦ˆ ì œê±°) í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    print(f"Loading data from: {file_path}")

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        raw_data = json.load(f)

    # ë„êµ¬ ì´ˆê¸°í™” (í…ìŠ¤íŠ¸ ì •ì œìš©)
    ht = HarvestText()

    cleaned_data = []

    for item in tqdm(raw_data, desc="Preprocessing"):
        content = item.get("content", "")
        label = item.get("label", None)

        if not content:
            continue

        # [ìˆ˜ì •ë¨] (1) ë²ˆì²´ -> ê°„ì²´ (zhconv ì‚¬ìš©)
        # 'zh-cn' ì˜µì…˜ì€ ì¤‘êµ­ ë³¸í†  ê°„ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        content = zhconv.convert(content, "zh-cn")

        # (2) ë…¸ì´ì¦ˆ ë° URL ì œê±° (HarvestText ì‚¬ìš©)
        content = ht.clean_text(content, emoji=False)
        content = remove_url(content)

        if not content.strip():
            continue

        # (3) ê²°ê³¼ ì €ì¥
        clean_item = {"content": content}
        if label:
            clean_item["label"] = label

        cleaned_data.append(clean_item)

    print(f"Processed {len(cleaned_data)} samples.")
    return cleaned_data


class EmotionDataset(Dataset):
    def __init__(self, data_list, tokenizer, label_map, max_len=128):
        self.data = data_list
        self.tokenizer = tokenizer
        self.label_map = label_map
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        text = item["content"]
        label_str = item.get("label", None)

        # í† í°í™” (Text -> Input IDs)
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_attention_mask=True,
            return_tensors="pt",
        )

        result = {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
        }

        # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
        if label_str:
            result["labels"] = torch.tensor(self.label_map[label_str], dtype=torch.long)

        return result


def train_epoch(model, data_loader, optimizer, device, epoch):
    model.train()
    total_loss = 0
    progress_bar = tqdm(data_loader, desc=f"Training Epoch {epoch + 1}")

    for batch in progress_bar:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

        progress_bar.set_postfix({"loss": loss.item()})

    return total_loss / len(data_loader)


def evaluate(model, data_loader, device):
    model.eval()
    preds = []
    true_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            _, prediction = torch.max(outputs.logits, dim=1)

            preds.extend(prediction.cpu().tolist())
            true_labels.extend(labels.cpu().tolist())

    acc = accuracy_score(true_labels, preds)
    f1 = f1_score(true_labels, preds, average="weighted")

    return acc, f1


def get_all_predictions(model, data_loader, device):
    """ì‹œê°í™”ë¥¼ ìœ„í•´ ëª¨ë¸ì˜ ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼(í™•ë¥  í¬í•¨) ì¶”ì¶œ"""
    model.eval()
    all_probs = []
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Visualizing"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probs = F.softmax(logits, dim=1)  # í™•ë¥  ê³„ì‚°
            _, preds = torch.max(logits, dim=1)

            all_probs.extend(probs.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_labels), np.array(all_preds), np.array(all_probs)


def plot_learning_curve(history):
    """1. í•™ìŠµ ê³¡ì„  (Learning Curve)"""
    epochs = range(1, len(history["train_loss"]) + 1)
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history["train_loss"], "b-o", label="Training Loss")
    plt.title("Training Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(epochs, history["val_f1"], "r-o", label="Validation F1")
    plt.title("Validation Weighted F1 Score")
    plt.xlabel("Epochs")
    plt.ylabel("F1 Score")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig(config.SAVE_PREFIX / "learning_curve.png")
    print("ğŸ“ˆ í•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ: learning_curve.png")


def plot_confusion_matrix_custom(y_true, y_pred, id2label):
    """2. í˜¼ë™ í–‰ë ¬ (Confusion Matrix)"""
    cm = confusion_matrix(y_true, y_pred)
    label_names = [id2label[i] for i in range(len(id2label))]

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=label_names,
        yticklabels=label_names,
    )
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title("Confusion Matrix (BERT)")
    plt.savefig(config.SAVE_PREFIX / "confusion_matrix.png")
    print("ğŸ”¥ í˜¼ë™ í–‰ë ¬ ì €ì¥ ì™„ë£Œ: confusion_matrix.png")


def plot_multiclass_roc(y_true, y_probs, num_classes, id2label):
    """3. ë‹¤ì¤‘ í´ë˜ìŠ¤ ROC ê³¡ì„  (One-vs-Rest)"""
    # ì •ë‹µ ë¼ë²¨ì„ ì›-í•« ì¸ì½”ë”©
    y_true_bin = label_binarize(y_true, classes=range(num_classes))

    fpr, tpr, roc_auc = dict(), dict(), dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    plt.figure(figsize=(10, 8))
    colors = cycle(
        ["blue", "red", "green", "orange", "purple", "cyan", "brown", "pink"]
    )

    for i, color in zip(range(num_classes), colors):
        label_name = id2label[i]
        plt.plot(
            fpr[i],
            tpr[i],
            color=color,
            lw=2,
            label=f"{label_name} (AUC = {roc_auc[i]:.2f})",
        )

    plt.plot([0, 1], [0, 1], "k--", lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Multi-class ROC Curve")
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.savefig(config.SAVE_PREFIX / "roc_curve.png")
    print("ROC ê³¡ì„  ì €ì¥ ì™„ë£Œ: roc_curve.png")


if __name__ == "__main__":
    print(f"Using Device: {config.DEVICE}")

    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    try:
        train_data = preprocess_data(config.TRAIN_FILE)
        eval_data = preprocess_data(config.EVAL_FILE)
    except Exception as e:
        print(f"Error during preprocessing: {e}")
        exit()

    # 2. ë¼ë²¨ ìë™ ê°ì§€ ë° ë§µí•‘
    all_labels = sorted(list(set([d["label"] for d in train_data])))
    label2id = {label: i for i, label in enumerate(all_labels)}
    id2label = {i: label for label, i in label2id.items()}

    print(f"Detected Labels ({len(all_labels)}): {label2id}")
    config.NUM_LABELS = len(all_labels)

    # 3. í† í¬ë‚˜ì´ì € & ë°ì´í„°ì…‹ ì¤€ë¹„
    tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)
    train_dataset = EmotionDataset(train_data, tokenizer, label2id, config.MAX_LEN)
    eval_dataset = EmotionDataset(eval_data, tokenizer, label2id, config.MAX_LEN)

    # 4. ë°ì´í„°ë¡œë”
    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
    eval_loader = DataLoader(eval_dataset, batch_size=config.BATCH_SIZE, shuffle=False)

    # 5. ëª¨ë¸ ì´ˆê¸°í™”
    print(f"Loading Model: {config.MODEL_NAME}...")
    model = BertForSequenceClassification.from_pretrained(
        config.MODEL_NAME, num_labels=config.NUM_LABELS
    )
    model.to(config.DEVICE)
    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)

    # 6. í•™ìŠµ ë£¨í”„
    history = {"train_loss": [], "val_acc": [], "val_f1": []}
    best_f1 = 0.0

    print("\nStart Training...")
    for epoch in range(config.EPOCHS):
        train_loss = train_epoch(model, train_loader, optimizer, config.DEVICE, epoch)
        acc, f1 = evaluate(model, eval_loader, config.DEVICE)

        history["train_loss"].append(train_loss)
        history["val_acc"].append(acc)
        history["val_f1"].append(f1)

        print(
            f"Epoch {epoch + 1}/{config.EPOCHS} | Loss: {train_loss:.4f} | Acc: {acc:.4f} | F1: {f1:.4f}"
        )

        # ìµœê³  ê¸°ë¡ ê°±ì‹  ì‹œ ì €ì¥
        if f1 > best_f1:
            best_f1 = f1
            torch.save(model.state_dict(), config.SAVE_PREFIX / "best.pt")
            print(f"ğŸ’¾ Best Model Saved! (F1: {best_f1:.4f})")

        print("-" * 30)

    print(f"\nğŸ‰ Training Complete. Best F1 Score: {best_f1:.4f}")
    print("\nGenerating Visualizations...")

    # (1) í•™ìŠµ ê³¡ì„ 
    plot_learning_curve(history)

    # (2) ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ (í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°)
    print("Loading best model for analysis...")
    model.load_state_dict(torch.load(config.SAVE_PREFIX / "best.pt"))
    model.to(config.DEVICE)

    # (3) ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
    y_true, y_pred, y_probs = get_all_predictions(model, eval_loader, config.DEVICE)

    # (4) í˜¼ë™ í–‰ë ¬ & ROC ê³¡ì„  ê·¸ë¦¬ê¸°
    plot_confusion_matrix_custom(y_true, y_pred, id2label)
    plot_multiclass_roc(y_true, y_probs, config.NUM_LABELS, id2label)

    print("\nâœ¨ All Done! ê²°ê³¼ ì´ë¯¸ì§€ì™€ best.pt íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
